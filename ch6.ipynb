{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강법\n",
    "\n",
    "## 경사하강법 실습\n",
    "\n",
    "파이토치는 경사하강법을 위한 자동편미분 기능 제공\n",
    "\n",
    "이 기능을 통해 경사하강법을 구현할 수 있다.\n",
    "\n",
    "여기서는 경사하강법을 통해 랜덤하게 생성된 텐서가 특정 텐서 값을 근사계산하도록 구현해볼 것. 여기에서 함수의 출력은 목표 텐서와 랜덤 텐서 사이의 차이가 될 것이고 함수의 입력은 랜덤 생성한 텐서의 현재 값이 될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000, 0.9000])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "target = torch.FloatTensor([i/10 for i in range(1, 10)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 값을 갖는 텐서를 하나 생성한다. 이 텐서의 requires_grad 속성이 True가 되도록 설정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6670, 0.4168, 0.7871, 0.0563, 0.0195, 0.4457, 0.2058, 0.4862, 0.3601],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand_like(target) # x로 미분한 final scalar\n",
    "x.requires_grad = True\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1792, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.mse_loss(x, target)\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while 반복문을 사용하여 두 텐서 값의 차이가 변수 threshold의 값보다 작아질 때까지 미분 및 경사하강법을 반복 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss : 1.0841e-01\n",
      "tensor([0.5410, 0.3686, 0.6789, 0.1327, 0.1263, 0.4800, 0.3156, 0.5559, 0.4801],\n",
      "       requires_grad=True)\n",
      "2-th Loss : 6.5579e-02\n",
      "tensor([0.4430, 0.3311, 0.5947, 0.1921, 0.2093, 0.5067, 0.4010, 0.6102, 0.5734],\n",
      "       requires_grad=True)\n",
      "3-th Loss : 3.9671e-02\n",
      "tensor([0.3668, 0.3020, 0.5292, 0.2383, 0.2739, 0.5274, 0.4675, 0.6524, 0.6460],\n",
      "       requires_grad=True)\n",
      "4-th Loss : 2.3999e-02\n",
      "tensor([0.3075, 0.2793, 0.4783, 0.2742, 0.3242, 0.5435, 0.5191, 0.6852, 0.7024],\n",
      "       requires_grad=True)\n",
      "5-th Loss : 1.4518e-02\n",
      "tensor([0.2614, 0.2617, 0.4386, 0.3022, 0.3632, 0.5561, 0.5593, 0.7107, 0.7463],\n",
      "       requires_grad=True)\n",
      "6-th Loss : 8.7824e-03\n",
      "tensor([0.2255, 0.2480, 0.4078, 0.3239, 0.3936, 0.5658, 0.5906, 0.7305, 0.7805],\n",
      "       requires_grad=True)\n",
      "7-th Loss : 5.3128e-03\n",
      "tensor([0.1976, 0.2373, 0.3839, 0.3408, 0.4173, 0.5734, 0.6149, 0.7460, 0.8070],\n",
      "       requires_grad=True)\n",
      "8-th Loss : 3.2139e-03\n",
      "tensor([0.1759, 0.2290, 0.3652, 0.3540, 0.4356, 0.5793, 0.6338, 0.7580, 0.8277],\n",
      "       requires_grad=True)\n",
      "9-th Loss : 1.9442e-03\n",
      "tensor([0.1591, 0.2226, 0.3507, 0.3642, 0.4499, 0.5839, 0.6485, 0.7673, 0.8438],\n",
      "       requires_grad=True)\n",
      "10-th Loss : 1.1761e-03\n",
      "tensor([0.1459, 0.2176, 0.3395, 0.3722, 0.4611, 0.5875, 0.6600, 0.7746, 0.8563],\n",
      "       requires_grad=True)\n",
      "11-th Loss : 7.1149e-04\n",
      "tensor([0.1357, 0.2137, 0.3307, 0.3783, 0.4697, 0.5903, 0.6689, 0.7802, 0.8660],\n",
      "       requires_grad=True)\n",
      "12-th Loss : 4.3041e-04\n",
      "tensor([0.1278, 0.2106, 0.3239, 0.3832, 0.4765, 0.5924, 0.6758, 0.7846, 0.8735],\n",
      "       requires_grad=True)\n",
      "13-th Loss : 2.6037e-04\n",
      "tensor([0.1216, 0.2083, 0.3186, 0.3869, 0.4817, 0.5941, 0.6812, 0.7880, 0.8794],\n",
      "       requires_grad=True)\n",
      "14-th Loss : 1.5751e-04\n",
      "tensor([0.1168, 0.2064, 0.3144, 0.3898, 0.4858, 0.5954, 0.6853, 0.7907, 0.8840],\n",
      "       requires_grad=True)\n",
      "15-th Loss : 9.5282e-05\n",
      "tensor([0.1131, 0.2050, 0.3112, 0.3921, 0.4889, 0.5964, 0.6886, 0.7928, 0.8876],\n",
      "       requires_grad=True)\n",
      "16-th Loss : 5.7640e-05\n",
      "tensor([0.1102, 0.2039, 0.3087, 0.3938, 0.4914, 0.5972, 0.6911, 0.7944, 0.8903],\n",
      "       requires_grad=True)\n",
      "17-th Loss : 3.4868e-05\n",
      "tensor([0.1079, 0.2030, 0.3068, 0.3952, 0.4933, 0.5978, 0.6931, 0.7956, 0.8925],\n",
      "       requires_grad=True)\n",
      "18-th Loss : 2.1093e-05\n",
      "tensor([0.1062, 0.2024, 0.3053, 0.3963, 0.4948, 0.5983, 0.6946, 0.7966, 0.8941],\n",
      "       requires_grad=True)\n",
      "19-th Loss : 1.2760e-05\n",
      "tensor([0.1048, 0.2018, 0.3041, 0.3971, 0.4959, 0.5987, 0.6958, 0.7974, 0.8954],\n",
      "       requires_grad=True)\n",
      "20-th Loss : 7.7190e-06\n",
      "tensor([0.1037, 0.2014, 0.3032, 0.3977, 0.4968, 0.5990, 0.6968, 0.7979, 0.8965],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "learning_rate = 1.\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold:\n",
    "    iter_cnt += 1\n",
    "    loss.backward() # 역전파로 편미분 수행\n",
    "    x = x - learning_rate * x.grad # learning rate에서 x를 업데이트\n",
    "    x.detach_()\n",
    "    x.requires_grad_()\n",
    "\n",
    "    loss = F.mse_loss(x, target)\n",
    "    \n",
    "    print('%d-th Loss : %.4e' % (iter_cnt, loss))\n",
    "    print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward 함수를 통해 편미분을 수행한다.\n",
    "\n",
    "편미분을 통해 얻어진 gradient들이 x_grad에 자동으로 저장되고, 이 값을 활용하여 경사하강법 수행.\n",
    "\n",
    "backward를 호출하기 위한 텐서의 크기는 **scalar**임에 주의.\n",
    "\n",
    "손실 값이 점차 줄어드는 것을 볼 수 있고, 텐서 $x$의 값이 목표 텐서 값에 근접해가는 것을 알 수 있다.\n",
    "\n",
    "학습률 변수를 조정한다면 tensor x가 목표 tensor에 근접해 가는 속도가 달라질 수 있다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치 autograd\n",
    "\n",
    "딥러닝은 경사하강법을 통해 학습되므로 미분이 필수. 파이토치는 **오토그래드라는 자동 미분 기능을 제공**한다.\n",
    "\n",
    "파이토치는 requires_grad 속성이 True인 텐서의 연산을 추적하기 위한 계산 그래프를 구축하고, backward 함수가 호출되면 이 그래프를 따라 미분으로 자동으로 수행하고 계산된 gradient를 채워 놓는다.\n",
    "\n",
    "다음과 같이 requires_grad 속성을 True로 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor([i for i in range(1,5)]).reshape(2,2).requires_grad_(True)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requires_grad 속성이 True인 텐서가 있을 때 이 텐서가 들어간 연산의 결과가 담긴 텐서도 자동으로 requires_grad 속성값을 True로 갖게 된다.\n",
    "\n",
    "다음 코드와 같이 여러 가지 연산을 수행하였을 때 결과 텐서들은 **모두 requires_grad 속성값을 가지게 된다**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x1 = x+2\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x2 = x - 2\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.,  0.],\n",
      "        [ 5., 12.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x3 = x1 * x2\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x.sum()\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 텐서들이 grad_fn 속성을 갖는다는 점을 주목하자.\n",
    "\n",
    "예를 들어 텐서 x1이 덧셈 연산의 결과물이기 때문에 x1의 grad_fn 속성이 AddBackward0임을 볼 수 있다.\n",
    "\n",
    "텐서 y는 sum 함수를 썼으므로 스칼라 값이 되었다. 여기에서 backward 함수를 호출해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 $x, x1, x2, x3, y$ 모두 grad 속성에 gradient 값이 계산되어 저장되었을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c084588ddc99ecfa5893987730956e72e2b095d02e836974b6e39a38a952fd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
