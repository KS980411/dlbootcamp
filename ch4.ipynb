{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형 계층"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 행렬 곱"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치에서 행렬 곱을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.FloatTensor([i for i in range(1, 7)]).reshape(3,2)\n",
    "y = torch.FloatTensor([[1, 2], [1,2]])\n",
    "\n",
    "print(x.size(), y.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치의 matmul 함수를 이용하면 행렬 곱을 수행할 수 있다.\n",
    "\n",
    "다음 코드는 앞서 만든 $x$와 $y$의 행렬 곱을 수행하고, 결과 행렬의 크기를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, y)\n",
    "print(z.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 행렬 곱\n",
    "\n",
    "딥러닝을 수행할 때 **보통 여러 샘플을 동시에 병렬 계산**하곤 한다. 따라서 행렬 곱 연산에도 여러 곱셈을 동시에 진행할 수 있어야 한다.\n",
    "\n",
    "bmm 함수가 이 역할을 수행한다. 다음 코드와 같이 텐서를 선언해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(3,3,2)\n",
    "y = torch.FloatTensor(3,2,3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서 bmm 함수를 활용하여 행렬 곱이 3번 수행되는 연산을 병렬로 동시에 진행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "z = torch.bmm(x, y)\n",
    "print(z.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bmm 함수는 마지막 2개의 차원을 행렬 취급하여 병렬로 행렬 곱 연산을 수행한다.\n",
    "\n",
    "bmm 함수를 적용하기 위해서는 마지막 2개 차원을 제외한 다른 차원의 크기는 동일해야 한다. 즉, 배치의 크기가 같아야 한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형 계층\n",
    "\n",
    "데이터를 모아 어떠한 함수를 근사계산하고 싶을 때, 어떤 모델로 특정 함수를 근사계산할 수 있을까? 이 절에서는 그 기본 모델이 될 수 있는 **선형 계층**에 대해서 다뤄볼 것이다.\n",
    "\n",
    "선형 계층은 심층신경망의 기본 구성 요소이고, 하나의 모델로도 동작할 수 있다.\n",
    "\n",
    "또한 입력과 출력이 다르게 반환할 수도 있다. 예를 들어 4차원의 실수 벡터를 입력받아 3차원의 실수 벡터를 반환하는 함수로 볼 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형 계층 함수의 동작 방식\n",
    "\n",
    "**가중치 파라미터**를 가지고 있으며, 이것에 의해 함수의 동작이 정의된다.\n",
    "\n",
    "각각의 출력 노드의 값은 입력 노드로부터 들어오는 값에 가중치 파라미터 $W_{i \\rightarrow j}$를 곱하고, 또 다른 가중치 파라미터 $b_{j}$를 더해서 결정한다.\n",
    "\n",
    "입력층이 4개, 출력층이 3개라면 $W$에는 따라서 총 12개의 가중치 파라미터가 존재한다.\n",
    "\n",
    "이 동작 방식은 다음과 같이 일반화 할 수 있다.\n",
    "\n",
    "$$ y = f(x) = W^{T} \\cdot x + b $$\n",
    "\n",
    "수 백만 개의 입력 벡터가 주어졌을 때, 이를 단순히 순차적으로 처리한다면 매우 비효율 적이다. 따라서 이 연산을 다수의 입력을 처리하기 위한 병렬 연산으로 생각해볼 수도 있다.\n",
    "\n",
    "$N$개의 $n$차원 벡터를 보아서 $N * n$의 행렬로 만들 수 있는데, 이것을 미니 배치라 한다.\n",
    "\n",
    "선형 계층 함수에서 미니배치 행렬을 처리하기 위한 수식은 다음과 같다.\n",
    "\n",
    "$$ y = f(x) = x \\cdot W + b$$\n",
    "\n",
    "입력을 $N$개 모아서 미니배치 행렬로 넣어주었기 때문에 출력도 $N$개의 $m$차원 벡터가 모여 $N * m$ 크기의 행렬이 된다.\n",
    "\n",
    "이와 같은 병렬 계산을 통해 연산 속도를 높일 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형 계층의 의미\n",
    "\n",
    "선형 계층은 **행렬 곱셈과 벡터의 덧셈으로 이루어져 있기 때문에 선형 변환**이라고 볼 수 이싿.\n",
    "\n",
    "선형 계층을 통해 모델을 구성할 경우, 선형 데이터에 대한 관계를 분석하거나 선형 함수는 근사계산할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형 계층 실습\n",
    "\n",
    "### 직접 구현하기\n",
    "\n",
    "선형 계층은 행렬 곱 연산과 브로드캐스팅 덧셈 연산으로 이루어져 있다.\n",
    "\n",
    "선형 계층의 파라미터 행렬 $W$가 행렬 곱 연산에 활용될 것이고, 파라미터 벡터 $b$가 브로드캐스팅 덧셈 연산에 활용될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.FloatTensor([i for i in range(1,7)]).reshape(3,2) # 가중치\n",
    "b = torch.FloatTensor([2,2]) # 편향\n",
    "\n",
    "def Linear(x, W,b):\n",
    "    y = torch.matmul(x, W) + b\n",
    "    return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 형식으로 선형 계층을 생성할 수 있다.\n",
    "\n",
    "하지만 이 방법은 파이토치 입장에서 제대로 된 계층(layer)으로 취급하지 않는다.\n",
    "\n",
    "제대로 된 계층을 만드는 방법을 지금부터 알아볼 것이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Module 클래스 상속 받기\n",
    "\n",
    "파이토치에는 nn(neural networks) 패키지가 있고, 내부에는 미리 정의된 많은 신경망들이 있다.\n",
    "\n",
    "그리고 그 신경망들은 torch.nn.Module이라는 **추상 클래스를 상속**받아 정의되어 있다.\n",
    "\n",
    "바로 이 추상 클래스를 상속받아 선형 계층을 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module을 상속받은 MyLinear라는 메서드를 정의할 것이다.\n",
    "\n",
    "nn.Module을 상속받은 클래스는 보통 2개의 method, __init__과 forward를 오버라이드한다(덮어쓴다).\n",
    "\n",
    "    __init__ 함수는 계층 내부에서 필요한 변수를 미리 선언하고 있으며 또 다른 계층을 소유하도록 할 수 있다.\n",
    "\n",
    "    forward 함수는 계층을 통과하는 데 필요한 계산 수행을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module): # 추상 클래스 상속\n",
    "    def __init__(self, input_dim = 3, output_dim = 2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        super().__init__() # nn_Module 클래스를 호출한다. 우리는 nn을 활용해서 신경망을 구성할 것이기 때문에 super으로 부모 클래스를 호출하는 것이 중요.\n",
    "        # 클래스를 인자로 넣을 때 자식클래스(부모클래스)로 인자를 삽입. 여기서 부모 클래스라는 것은 nn.Module이다. super을 통해 nn.Module의 인자들을 활용할 수 있다.\n",
    "        # 오버라이딩은 자식 클래스와 부모 클래스의 함수가 겹칠 때 자식 클래스의 함수가 부모 클래스의 함수를 덮어 쓴다는 것.\n",
    "        # 그렇다면 여기서의 super는 nn.Module의 함수들을 상속받되 __init__과 forward는 자식 클래스의 함수로 override하겠다는 의미로 해석할 수 있다.\n",
    "\n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim)) # 임의의 가중치 W 생성\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_dim)) # 임의의 가중치 b 생성\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.matmul(x, self.W) + self.b # W, b는 다음 함수에서 계속 써야하기 때문에 self로 지정\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 파라미터가 정상적으로 출력되는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-7.4488e-09,  2.0005e+00],\n",
      "        [ 0.0000e+00, -2.5244e-29],\n",
      "        [-2.0156e+00,  4.5730e-41]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.0489e-08, 1.2971e-11], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear = MyLinear()\n",
    "\n",
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear 활용하기\n",
    "\n",
    "torch.nn에 미리 정의된 선형 계층을 불러다 쓰면 간단하다. 다음 코드는 nn.Linear를 통해 선형 계층을 활용한 모습이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4367,  0.1939,  0.2293],\n",
      "        [-0.4986,  0.3862, -0.2407]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0548, -0.1736], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(3,2)\n",
    "\n",
    "x = torch.FloatTensor(1, 3)\n",
    "y = linear(x)\n",
    "\n",
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module을 상속받아 정의한 MyLinear 클래스는 내부의 nn.Module 하위 클래스를 소유할 수 있다.\n",
    "\n",
    "super을 통해 상속받았기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, input_dim = 3, output_dim = 2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim \n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module을 상속받아 MyLinear 클래스를 정의하고 있다.\n",
    "\n",
    "__init__ 함수 내부에 nn.Linear을 선언, self.linear에 저장한다.\n",
    "\n",
    "forward 함수에서는 self.linear에 텐서 $x$를 통과시킨다. 즉, 이 코드도 선형 계층을 구현한 것이라고 볼 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c084588ddc99ecfa5893987730956e72e2b095d02e836974b6e39a38a952fd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
